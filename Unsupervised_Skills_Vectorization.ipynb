{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Skills Vectorisation"
      ],
      "metadata": {
        "id": "9S5PpQqBFvHq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgRGnNDKzviB",
        "outputId": "98c63fa7-8034-43bc-88e9-8f4cf8102227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import re\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JDdZeRHBgW4S"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOFq3ApYmsCv",
        "outputId": "584558f9-37a3-41b9-c4c3-07673ffa01d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/.shortcut-targets-by-id/1CDppNxm5AHmRDwItmGGQo-T6INF5HrH_/IS4242\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/gdrive')\n",
        "drive_path = '/content/gdrive/MyDrive/IS4242'\n",
        "os.chdir(drive_path)\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MHMjLYy93c3A"
      },
      "outputs": [],
      "source": [
        "# drive_items = os.listdir(os.getcwd())\n",
        "# print(drive_items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6_XItkpqnT7Q"
      },
      "outputs": [],
      "source": [
        "#df = pd.read_csv('Supervised_Component/quickFE.csv')\n",
        "data_scientist_us_df = pd.read_csv('Supervised_Component/data_scientist_us.csv', encoding='iso-8859-1')\n",
        "data_scientist_sg_df = pd.read_csv('Supervised_Component/data_scientist_sg.csv', encoding='iso-8859-1')\n",
        "data_analyst_us_df = pd.read_csv('Supervised_Component/data_analyst_us.csv', encoding='iso-8859-1')\n",
        "data_analyst_sg_df = pd.read_csv('Supervised_Component/data_analyst_sg.csv', encoding='iso-8859-1')\n",
        "data_engineer_us_df = pd.read_csv('Supervised_Component/data_engineer_us.csv', encoding='iso-8859-1')\n",
        "data_engineer_sg_df = pd.read_csv('Supervised_Component/data_engineer_sg.csv', encoding='iso-8859-1')\n",
        "df_fe = pd.read_csv('Supervised_Component/final_dataset.csv', encoding='iso-8859-1') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_fe.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk1PLemHtOtn",
        "outputId": "d338e63c-44e3-465d-9789-d7f45b7b390f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['company_starRating', 'company_salary', 'listing_jobDesc', 'role_type',\n",
              "       'location', 'Monthly_Salary', 'cleaned_listing_jobDesc',\n",
              "       'country_binarylabel', 'tableau', 'data warehousing',\n",
              "       'machine learning', 'etl', 'power bi', 'scala', 'excel', 'hadoop',\n",
              "       'oracle', 'r', 'data pipeline', 'spark', 'sql', 'data governance',\n",
              "       'data engineering', 'snowflake', 'optimization', 'git', 'powerpoint',\n",
              "       'nosql', 'data quality', 'data visualization', 'data mining', 'aws',\n",
              "       'business intelligence', 'data engineer', 'agile', 'java', 'rest',\n",
              "       'query', 'data analyst', 'python', 'data scientist',\n",
              "       'project management', 'big data', 'data modeling', 'azure', 'api',\n",
              "       'unity'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGK_NJZQYWQX",
        "outputId": "f48d02f4-b316-4a29-d968-b7bc67ce7775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--data analyst--\n",
            "US total: 4678, Unique JD: 1790\n",
            "SG total: 1003, Unique JD: 381\n",
            "--data scientist--\n",
            "US total: 609, Unique JD: 609\n",
            "SG total: 1038, Unique JD: 246\n",
            "--data engineer--\n",
            "US total: 2398, Unique JD: 995\n",
            "SG total: 1875, Unique JD: 876\n"
          ]
        }
      ],
      "source": [
        "print(\"--data analyst--\")\n",
        "data_analyst_us_df_2 = data_analyst_us_df.drop_duplicates(subset=['listing_jobDesc'])#[:150]\n",
        "data_analyst_sg_df_2 = data_analyst_sg_df.drop_duplicates(subset=['listing_jobDesc'])#[:150]\n",
        "print(\"US total: {}, Unique JD: {}\".format(len(data_analyst_us_df), len(data_analyst_us_df_2)))\n",
        "print(\"SG total: {}, Unique JD: {}\".format(len(data_analyst_sg_df), len(data_analyst_sg_df_2)))\n",
        "\n",
        "print(\"--data scientist--\")\n",
        "data_scientist_us_df_2 = data_scientist_us_df.drop_duplicates(subset=['listing_jobDesc'])#[:150]\n",
        "data_scientist_sg_df_2 = data_scientist_sg_df.drop_duplicates(subset=['listing_jobDesc'])#[:150]\n",
        "print(\"US total: {}, Unique JD: {}\".format(len(data_scientist_us_df), len(data_scientist_us_df_2)))\n",
        "print(\"SG total: {}, Unique JD: {}\".format(len(data_scientist_sg_df), len(data_scientist_sg_df_2)))\n",
        "\n",
        "print(\"--data engineer--\")\n",
        "data_engineer_us_df_2 = data_engineer_us_df.drop_duplicates(subset=['listing_jobDesc'])#[:150]\n",
        "data_engineer_sg_df_2 = data_engineer_sg_df.drop_duplicates(subset=['listing_jobDesc'])#[:150]\n",
        "print(\"US total: {}, Unique JD: {}\".format(len(data_engineer_us_df), len(data_engineer_us_df_2)))\n",
        "print(\"SG total: {}, Unique JD: {}\".format(len(data_engineer_sg_df), len(data_engineer_sg_df_2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "dbEtotTEYMQl"
      },
      "outputs": [],
      "source": [
        "data_scientist_us_df_2 = data_scientist_us_df_2[:150]\n",
        "data_scientist_sg_df_2 = data_scientist_sg_df_2[:150]\n",
        "data_analyst_us_df_2 = data_analyst_us_df_2[:150]\n",
        "data_analyst_sg_df_2 = data_analyst_sg_df_2[:150]\n",
        "data_engineer_us_df_2 = data_engineer_us_df_2[:150]\n",
        "data_engineer_sg_df_2 = data_engineer_sg_df_2[:150]\n",
        "\n",
        "combined_df = pd.concat([data_scientist_us_df_2,data_scientist_sg_df_2,data_analyst_us_df_2, data_analyst_sg_df_2, data_engineer_us_df_2, data_engineer_sg_df_2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDz1Odt7JAe6",
        "outputId": "61cb78bc-b640-43a8-d980-f2771317cb23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "900"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "len(combined_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--combined--\")\n",
        "combined_df_2 = combined_df.drop_duplicates(subset=['listing_jobDesc'])\n",
        "print(\"Combined total: {}, Unique JD: {}\".format(len(combined_df), len(combined_df_2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5xu3h-TB5Vi",
        "outputId": "fbba237f-5d82-4b78-dbb8-1806bedf0019"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--combined--\n",
            "Combined total: 900, Unique JD: 845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbRCiHmp4v_s"
      },
      "source": [
        "# Zero Shot Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm_gEi3m4qv6",
        "outputId": "b98b2de2-4d7f-4e6b-e6b6-6d22a882ad57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "bCrp71Zm4l96"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)\n",
        "\n",
        "# pipe = pipeline(model=\"facebook/bart-large-mnli\")\n",
        "# pipe(\"I have a problem with my iphone that needs to be resolved asap!\",\n",
        "#     candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO5FBAZJ5AFG"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "E297yktJ7JoR"
      },
      "outputs": [],
      "source": [
        "job_description = combined_df_2.iloc[0]['listing_jobDesc']\n",
        "# skills_lst = ['sql','python','nosql','postgresql','redshift','athena','tableau','power bi','excel','data visualization','dashboards','machine learning','scala','java','javascript','html','c++','hive','aws','tensorflow','pytorch','flask','plumber','ai','keras','matlab','data scraping', 'nlp','powerpoint','etl','azure','linux','spark','hadoop','docker','kubernetes','numpy','postgresql','query']\n",
        "# results = classifier(job_description, skills_lst)\n",
        "# print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6s4hOUn8UEj"
      },
      "source": [
        "### Very slow\n",
        "1. Reduce size of skill list\n",
        "2. Use smaller model, eg BERT based\n",
        "3. Shorten the JD. remove stop words,stemming/ lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "AG6d3YTL-OHo"
      },
      "outputs": [],
      "source": [
        "def job_description_preprocessing(job_description):\n",
        "  job_description = job_description.lower()\n",
        "  job_description = ''.join(c for c in job_description if c.isalnum() or c.isspace())\n",
        "  words = word_tokenize(job_description)\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  words = [word for word in words if word not in stop_words]\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  words = [lemmatizer.lemmatize(word) for word in words]\n",
        "  return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "YCVqfKrfe0tq"
      },
      "outputs": [],
      "source": [
        "def get_skills_vectors(job_description, skills_lst):\n",
        "  result = classifier(job_description, skills_lst, multi_label=True)\n",
        "  skills_dict = dict(zip(result['labels'], result['scores']))\n",
        "  return skills_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG3aNxLk-hX0",
        "outputId": "8cbd6828-c6fd-4d9c-c9f8-8565f9a974b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "execute sampling weighting plan various probability nonprobability survey effort ranging regional survey smaller survey effort targeting specific population perform analysis large data set translate finding digestible information help inform various agency project research collect compile data ensure accuracy information maintain database documentation perform complex statistical method analysis quasiexperimental research design nonequivalent group eg regression adjustment matching propensity score stratification longitudinal analysis classification dimension reduction clustering hierarchical linear random effect modeling various evaluation project perform various duty related development maintenance improvement sandags abm rsm cvm cbm perform research participate development modeling feature related emerging transportation technology transformative mode shared mobility ensure transportation modeling tool analytical capability credible effective policy analysis participate quality assurance quality control process coordinate support participate project matrix team assembled transportation modeling analysis required longrange regional plan highway transit investment study coordinate applied research division maintain transportation database ensure current accurate traffic passenger count data used prepare present written oral visual report policy stakeholder committee member agency community group private organization member public develop request proposal scope work project budget evaluate competitive project proposal make recommendation consultant selection manage project consultant ensure project completed schedule within budget highlevel effectiveness supervise mentor train provide career development opportunity professional staff bachelor degree statistic mathematics transportation modeling computer science engineering related field emphasis survey methodology preferred graduate degree preferred five year career professional work experience sample design sampling implementation weighting nonresponse analysisadjustments missing data technique questionnaire design assessingminimizing survey bias variance error knowledge principle practice travel demand modeling forecasting experience development activitybased model desirable experience programming language java python r sql experience developing maintaining custom software application support modeling function proficiency statistical software package spss r python required knowledge experience relational database sql server including writing query transform summarize data stored procedure data view familiar microsoft sql server enterprise relational database system experience bigpassive data application related transportation system performance household point level business socioeconomic data mobility service data apis knowledge azure cloud computing environment desirable experience data visualization tool power bi tableau esri story map preferred experience supervising evaluating work employee experience evaluating internal business process balancing workload staff resource ability delegate authority responsibility strong communication analytical skill support business leader decision maker excellent organizational skill attention detail ability maintain high level accuracy ability work effectively independently team member hybrid work option 980 flexible work schedule payforperformance merit increase calpers pension plan employer employee contribution excellent health insurance option employee eligible dependent free dental vision insurance employee eligible dependent education assistance 5250 year regular employee free transit pas use throughout san diego region bus rapid express trolley coaster paid time including 12 paid holiday 2 floating holiday day generous paid time pto per year depending length service\n"
          ]
        }
      ],
      "source": [
        "job_description = job_description_preprocessing(job_description)\n",
        "print(job_description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87MojoCo-LJA",
        "outputId": "64fab74d-3b36-4752-ad97-ac8c51688bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39\n"
          ]
        }
      ],
      "source": [
        "skills_lst = [\n",
        "    'tableau', 'data warehousing',\n",
        "    'machine learning', 'etl', 'power bi', \n",
        "    'scala', 'excel', 'hadoop', 'oracle', \n",
        "    'r', 'data pipeline', 'spark', 'sql', \n",
        "    'data governance', 'data engineering', \n",
        "    'snowflake', 'optimization', 'git', \n",
        "    'powerpoint', 'nosql', 'data quality', \n",
        "    'data visualization', 'data mining', \n",
        "    'aws', 'business intelligence', \n",
        "    'data engineer', 'agile', 'java', \n",
        "    'rest', 'query', 'data analyst', \n",
        "    'python', 'data scientist', \n",
        "    'project management', 'big data', \n",
        "    'data modeling', 'azure', 'api', \n",
        "    'unity'\n",
        "    ]\n",
        "       \n",
        "print(len(skills_lst))\n",
        "results_2 = classifier(job_description, skills_lst, multi_label=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "E3-d91Sk9gXU",
        "outputId": "580fd5b7-f3da-40c3-b0b1-1213afad4f38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'execute sampling weighting plan various probability nonprobability survey effort ranging regional survey smaller survey effort targeting specific population perform analysis large data set translate finding digestible information help inform various agency project research collect compile data ensure accuracy information maintain database documentation perform complex statistical method analysis quasiexperimental research design nonequivalent group eg regression adjustment matching propensity score stratification longitudinal analysis classification dimension reduction clustering hierarchical linear random effect modeling various evaluation project perform various duty related development maintenance improvement sandags abm rsm cvm cbm perform research participate development modeling feature related emerging transportation technology transformative mode shared mobility ensure transportation modeling tool analytical capability credible effective policy analysis participate quality assurance quality control process coordinate support participate project matrix team assembled transportation modeling analysis required longrange regional plan highway transit investment study coordinate applied research division maintain transportation database ensure current accurate traffic passenger count data used prepare present written oral visual report policy stakeholder committee member agency community group private organization member public develop request proposal scope work project budget evaluate competitive project proposal make recommendation consultant selection manage project consultant ensure project completed schedule within budget highlevel effectiveness supervise mentor train provide career development opportunity professional staff bachelor degree statistic mathematics transportation modeling computer science engineering related field emphasis survey methodology preferred graduate degree preferred five year career professional work experience sample design sampling implementation weighting nonresponse analysisadjustments missing data technique questionnaire design assessingminimizing survey bias variance error knowledge principle practice travel demand modeling forecasting experience development activitybased model desirable experience programming language java python r sql experience developing maintaining custom software application support modeling function proficiency statistical software package spss r python required knowledge experience relational database sql server including writing query transform summarize data stored procedure data view familiar microsoft sql server enterprise relational database system experience bigpassive data application related transportation system performance household point level business socioeconomic data mobility service data apis knowledge azure cloud computing environment desirable experience data visualization tool power bi tableau esri story map preferred experience supervising evaluating work employee experience evaluating internal business process balancing workload staff resource ability delegate authority responsibility strong communication analytical skill support business leader decision maker excellent organizational skill attention detail ability maintain high level accuracy ability work effectively independently team member hybrid work option 980 flexible work schedule payforperformance merit increase calpers pension plan employer employee contribution excellent health insurance option employee eligible dependent free dental vision insurance employee eligible dependent education assistance 5250 year regular employee free transit pas use throughout san diego region bus rapid express trolley coaster paid time including 12 paid holiday 2 floating holiday day generous paid time pto per year depending length service'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "results_2['sequence']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyN7SwtP4-1N",
        "outputId": "8319c4b2-271c-4412-c6d7-7e25b92ca974"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'azure': 0.9887955188751221,\n",
              " 'data visualization': 0.9819573163986206,\n",
              " 'tableau': 0.9804214239120483,\n",
              " 'data modeling': 0.9799231886863708,\n",
              " 'power bi': 0.9796057939529419,\n",
              " 'java': 0.9790242910385132,\n",
              " 'project management': 0.9774934649467468,\n",
              " 'query': 0.972189724445343,\n",
              " 'python': 0.9712331295013428,\n",
              " 'r': 0.9672112464904785,\n",
              " 'sql': 0.9653732776641846,\n",
              " 'spark': 0.9614583849906921,\n",
              " 'data governance': 0.9581291079521179,\n",
              " 'data quality': 0.9547718167304993,\n",
              " 'etl': 0.9406257271766663,\n",
              " 'data analyst': 0.9336547255516052,\n",
              " 'big data': 0.9272109866142273,\n",
              " 'aws': 0.9171643853187561,\n",
              " 'data scientist': 0.886657178401947,\n",
              " 'hadoop': 0.8814746737480164,\n",
              " 'excel': 0.8551022410392761,\n",
              " 'data engineering': 0.8486582636833191,\n",
              " 'api': 0.8277952671051025,\n",
              " 'nosql': 0.8049618601799011,\n",
              " 'business intelligence': 0.7767814993858337,\n",
              " 'oracle': 0.7665531039237976,\n",
              " 'rest': 0.7481712102890015,\n",
              " 'data engineer': 0.7451900243759155,\n",
              " 'git': 0.7262341976165771,\n",
              " 'agile': 0.7113373875617981,\n",
              " 'unity': 0.6969960927963257,\n",
              " 'data pipeline': 0.6911473870277405,\n",
              " 'optimization': 0.6721325516700745,\n",
              " 'data warehousing': 0.639940083026886,\n",
              " 'scala': 0.5925103425979614,\n",
              " 'powerpoint': 0.5710297226905823,\n",
              " 'data mining': 0.5512334704399109,\n",
              " 'machine learning': 0.5050992369651794,\n",
              " 'snowflake': 0.2824648916721344}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "skills_dict = dict(zip(results_2['labels'], results_2['scores']))\n",
        "skills_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdoXHLA1hPQe"
      },
      "source": [
        "# Processing start - Actual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "BZYNN9SOgo3E"
      },
      "outputs": [],
      "source": [
        "combined_df_2['cleaned_jd'] = combined_df_2['listing_jobDesc'].apply(lambda x: job_description_preprocessing(str(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "nAdH6qCAguum"
      },
      "outputs": [],
      "source": [
        "combined_df_2['skills_vec'] = combined_df_2['cleaned_jd'].apply(lambda x: get_skills_vectors(str(x), skills_lst))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "kUIi8WCYgvsM"
      },
      "outputs": [],
      "source": [
        "combined_df_2.to_csv('Skills Vector/Skills_Vec_BART_Combined_2.csv', encoding='utf-8', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPdobHk3Yb_y"
      },
      "source": [
        "# BERT extract keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPrR4bauZJVA"
      },
      "outputs": [],
      "source": [
        "# job_description = sample_df.iloc[0]['listing_jobDesc']\n",
        "# job_description = job_description_preprocessing(job_description)\n",
        "\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# # length of keyword\n",
        "# n_gram_range = (1, 1)\n",
        "\n",
        "# # for removing stopwords\n",
        "# stop_words = \"english\"\n",
        "\n",
        "# # Extract candidate words/phrases\n",
        "# count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([job_description])\n",
        "# candidates = count.get_feature_names_out()\n",
        "# print('Printing candidate keywords from this sample job description: ', candidates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVtxGRrKZLxW"
      },
      "outputs": [],
      "source": [
        "# !pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I73CF-WhZ0Q0"
      },
      "outputs": [],
      "source": [
        "# from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "# doc_embedding = model.encode([job_description])\n",
        "# #candidates = ['sql', 'python', 'nosql', 'postgresql', 'tableau', 'excel', 'data visualization', 'machine learning', 'java', 'javascript', 'html', 'hive', 'aws', 'tensorflow', 'pytorch', 'nlp', 'etl', 'azure', 'linux', 'spark', 'hadoop', 'docker', 'numpy']\n",
        "# candidates = ['sql', 'python', 'java', 'extract transform load (etl)', 'data modeling', 'data warehousing', 'data visualization',\n",
        "#                'excel', 'tableau', 'statistical analysis', 'regression analysis', 'machine learning',\n",
        "#                'deep learning', 'natural language processing', 'big data technologies', 'hadoop',\n",
        "#                'spark', 'amazon web services (aws)', 'azure', 'google cloud platform (gcp)', 'git', 'mathematics', 'statistics', 'cloud computing',\n",
        "#                'computer vision', 'tensorflow', 'pytorch'\n",
        "#                ]\n",
        "# candidate_embeddings = model.encode(candidates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKFvLKotamG7"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "# keywords = [candidates[index] for index in distances.argsort()[0]]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}